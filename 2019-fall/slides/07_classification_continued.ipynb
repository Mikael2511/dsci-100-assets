{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DSCI 100 - Introduction to Data Science\n",
    "\n",
    "\n",
    "### Lecture 7 - Classification II: Evaluating & Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housekeeping\n",
    "\n",
    "- Thanks for filling out the mid-course survey! We're collecting and processing responses.\n",
    "- Grades are posted for Tutorial 04, Quiz 1, and Worksheet 06\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuing with the classification problem\n",
    "\n",
    "**Recall:** cancer tumour cell data, with \"benign\" and \"malignant\" labels\n",
    "\n",
    "<center><img src=\"https://ubc-dsci.github.io/introduction-to-datascience/_main_files/figure-html/06-precode-1.png\" width=\"800\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Today: unanswered questions from last week\n",
    "\n",
    "1. Is our model any good? How do we evaluate it?\n",
    "\n",
    "2. How do we choose `k` in K-nearest neighbours classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "To add evaluation into our classification pipeline, we:\n",
    "\n",
    "1. Split our data into two subsets: *training data* and *testing data*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/training_test.jpeg\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "2. Build the model & choose K using **training data only**\n",
    "3. Compute accuracy by predicting labels on **testing data only**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/ML-paradigm-test.png\" width=\"1700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why?\n",
    "\n",
    "Showing your classifier the labels of evaluation data is like cheating on a test; it'll look more accurate than it really is<br>\n",
    "\n",
    "**Golden Rule of Machine Learning / Statistics:** *Don't use your testing data to train your model!*\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img width=\"800px\" src=\"https://media2.giphy.com/media/12vJgj7zMN3jPy/giphy.gif\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting Data\n",
    "\n",
    "There are two important things to do when splitting data.\n",
    "\n",
    "1. **Shuffling:** randomly reorder the data before splitting\n",
    "2. **Stratification:** make sure the two split subsets of data have roughly equal proportions of the different labels\n",
    "\n",
    "**Why?** Discuss in your groups for 1 minute!\n",
    "\n",
    "(`caret` thankfully automatically does both of these things)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing K (or, \"tuning'' the model)\n",
    "\n",
    "Want to choose K to maximize accuracy, but:\n",
    "- we can't use training data to evaluate accuracy (cheating!)\n",
    "- we can't use test data to evaluate accuracy (choosing K is part of training!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution:** Split the training data further into *training data* and *validation data*\n",
    "\n",
    "<br>2a. Choose some candidate values of K\n",
    "<br>2b. Train the model for each using **training data only**\n",
    "<br>2c. Evaluate accuracy for each using **validation data only**\n",
    "<br>2d. Pick the K that maximizes validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "We can get a better estimate of accuracy by splitting *multiple ways* and *averaging*\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/cv.png\" width=\"1400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "\n",
    "**Overfitting:** when your model is too sensitive to your training data; noise can influence predictions!\n",
    "\n",
    "**Underfitting:** when your model isn't sensitive enough to training data; useful information is ignored!\n",
    "\n",
    "\n",
    "<center>\n",
    "<img width=\"1200px\" src=\"https://ubc-dsci.github.io/introduction-to-datascience/_main_files/figure-html/06-decision-grid-K-1.png\"/>\n",
    "</center>\n",
    "\n",
    "**Which of these are under-, over-, and good fits?** Discuss in your groups for 1 minute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "For KNN: small K overfits, large K underfits, both cause poor accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/_main_files/figure-html/06-lots-of-ks-1.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Big Picture\n",
    "\n",
    "<center>\n",
    "<img src=\"img/big_picture.png\" width=\"1200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worksheet Time!\n",
    "\n",
    "## ...and if we've learned anything from last time,\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://blog.frogslayer.com/wp-content/uploads/2018/07/there-will-be-bugs-meme.jpg\" width=\"1000\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Activity\n",
    "\n",
    "In your group, discuss the following prompts. Post your group's answer on Piazza:\n",
    "- Explain what a test, validation and training data set are in your own words\n",
    "- Explain cross-validation in your own words\n",
    "- Imagine if we train *and* evaluate accuracy on all the data. **How can I get 100% accuracy, *always*?**\n",
    "- Why can't I use cross validation when testing?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
